\chapter{ESPACIOS NORMADOS}
\printchaptertableofcontents

%\section{Introducción}

Las primeras secciones de este capítulo introducen el concepto de espacio vectorial y exploran las propiedades elementales que resultan de su definición básica. Se desarrollan e ilustran con ejemplos las nociones de subespacio, independencia lineal, convexidad y dimensión. Este material es en gran parte una revisión para la mayoría de los lectores, ya que duplica la primera parte de los cursos estándar de álgebra lineal.

La segunda parte del capítulo analiza las propiedades básicas de los espacios lineales normados. Un espacio lineal normado es un espacio vectorial en el que se define una medida de distancia o longitud. Con la introducción de una norma, es posible definir propiedades analíticas o topológicas como la convergencia y los conjuntos abiertos y cerrados. Por lo tanto, esta parte del capítulo introduce y explora estos conceptos básicos que distinguen el análisis funcional del álgebra lineal.

\section{Espacios vectoriales}

\subsection{Definición y ejemplos}

Asociado a cada espacio vectorial hay un conjunto de escalares utilizados para definir la multiplicación escalar en el espacio. En el entorno más abstracto, estos escalares solo deben ser elementos de un cuerpo algebraico. Sin embargo, en este libro los escalares se toman siempre como el conjunto de los números reales o de los números complejos. A veces distinguimos entre estas posibilidades refiriéndonos a un espacio vectorial como un espacio vectorial real o complejo. No obstante, en este libro se enfatiza principalmente en los espacios vectoriales reales y, aunque ocasionalmente se hace referencia a los espacios complejos, muchos resultados se derivan solo para los espacios reales. En caso de ambigüedad, el lector debe suponer que los escalares son reales.

\begin{definicion}{}{espvec}
    Un \emph{espacio vectorial} $V$ es un conjunto de elementos llamados \emph{vectores} junto con dos operaciones. La primera operación es la \emph{suma}, la cual asocia a cualquier par de vectores $\mathbf{u}, \mathbf{v} \in V$ un vector suma $\mathbf{u} + \mathbf{v} \in V$. La segunda operación es la \emph{multiplicación escalar} o \emph{producto escalar}, la cual asocia a cualquier vector $\mathbf{u} \in V$ y cualquier escalar $\alpha \in K$ (donde $K$ es un \emph{campo}), un vector $\alpha \cdot \mathbf{u} \in V$. Dichas operaciones se pueden establecer como:
    \begin{alignat*}{2}
        + &: & \quad V \times V & \longrightarrow V \\
        & & (\mathbf{u}, \mathbf{v}) & \longmapsto \mathbf{u} + \mathbf{v} \\
        & \\
        \cdot &: & \quad K \times V & \longrightarrow V \\
        & & (\alpha, \mathbf{u}) & \longmapsto \alpha \cdot \mathbf{u}
    \end{alignat*}
    Decimos que $V$ junto con las operaciones de suma y multiplicación escalar, es un espacio vectorial sobre $K$, si cumple con los siguientes axiomas: Para toda $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w} \in V$ y $\alpha$, $\beta \in K$
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        %\item Cerradura: $\mathbf{u} + \mathbf{v} \in V$.
        \item Conmutatividad: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$.
        \item Asociatividad sobre la suma de vectores: $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$.
        \item Neutro aditivo: Existe un elemento $\mathbf{0} \in V$, que llamaremos el \emph{vector cero}, tal que $\mathbf{u} + \mathbf{0} = \mathbf{0} + \mathbf{u} = \mathbf{u}$.
        \item Inverso aditivo: Para cada vector $\mathbf{u} \in V$ existe un elemento $-\mathbf{u} \in V$, tal que $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$. A $-\mathbf{u}$ se le llama \emph{negativo} o \emph{inverso aditivo} de $\mathbf{u}$.
        %\item Cerradura: $\alpha \cdot \mathbf{u} \in V$.
        \item Distributividad sobre la suma de vectores: $\alpha \cdot (\mathbf{u} + \mathbf{v}) = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}$.
        \item Distributividad sobre la suma de escalares: $(\alpha + \beta) \cdot \mathbf{u} = \alpha \cdot \mathbf{u} + \beta \cdot \mathbf{u}$.
        \item Asociatividad sobre el producto escalar: $\alpha \cdot (\beta \cdot \mathbf{u}) = (\alpha \beta) \cdot \mathbf{u}$.
        \item Identidad multiplicativa: $1 \cdot \mathbf{u} = \mathbf{u}$.
    \end{enumerate}
\end{definicion}

\begin{theorem}{}{}
    Sea $V$ un espacio vectorial sobre $K$, entonces
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item $\alpha \cdot \mathbf{0} = \mathbf{0}$, $\forall \alpha \in K$.
        \item $0 \cdot \mathbf{u} = \mathbf{0}$, $\forall \mathbf{u} \in V$.
        \item Si $\alpha \cdot \mathbf{u} = \mathbf{0}$, entonces $\alpha = 0$ o $\mathbf{u} = \mathbf{0}$.
        \item $(-1) \cdot \mathbf{u} = - \mathbf{u}$.
    \end{enumerate}

    \tcblower
    \demostracion Solo demostraremos (i), los demás se dejan como ejercicio.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Sea $\alpha \in K$, tenemos que
        \begin{align}
            \mathbf{0} & = \mathbf{0} + \mathbf{0} && \text{por axioma iii)} \label{ec7} \\
            & = \alpha \cdot \mathbf{0} + (-\alpha \cdot \mathbf{0}) && \text{por axioma iv)} \label{ec8}
        \intertext{A partir de \eqref{ec7},}
            \alpha \cdot \mathbf{0} & = \alpha \cdot (\mathbf{0} + \mathbf{0}) && \text{por def. de producto escalar} \label{ec9} \\
            & = \alpha \cdot \mathbf{0} + \alpha \cdot \mathbf{0} && \text{por axioma v)} \label{ec10}
        \intertext{Ahora, sustituyendo \eqref{ec10} en \eqref{ec8}, se sigue que}
            \mathbf{0} & = (\alpha \cdot \mathbf{0} + \alpha \cdot \mathbf{0}) + (-\alpha \cdot \mathbf{0}) \notag \\
            & = \alpha \cdot \mathbf{0} + \big( \alpha \cdot \mathbf{0} + (-\alpha \cdot \mathbf{0}) \big) && \text{por axioma ii)} \notag \\
            & = \alpha \cdot \mathbf{0} + \mathbf{0} && \text{por axioma iii)} \notag \\
            & = \alpha \cdot \mathbf{0} && \text{por axioma iii)} \notag
        \end{align}
        Por lo tanto, se concluye que
        $$\alpha \cdot \mathbf{0} = \mathbf{0}.$$
    \end{enumerate}
\end{theorem}

\newpage

\begin{prop}{}{}
    En cualquier espacio vectorial:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        %\item Si $\mathbf{u} + \mathbf{v} = \mathbf{u} + \mathbf{w}$, entonces $\mathbf{v} = \mathbf{w}$.
        \item Si $\alpha \cdot \mathbf{u} = \alpha \cdot \mathbf{v}$ y $\alpha \neq 0$, entonces $\mathbf{u} = \mathbf{v}$.
        \item Si $\alpha \cdot \mathbf{u} = \beta \cdot \mathbf{u}$ y $\mathbf{u} \neq \mathbf{0}$, entonces $\alpha = \beta$.
        \item $(\alpha - \beta) \cdot \mathbf{u} = \alpha \cdot \mathbf{u} - \beta \cdot \mathbf{u}$.
        \item $\alpha \cdot (\mathbf{u} - \mathbf{v}) = \alpha \cdot \mathbf{u} - \alpha \cdot \mathbf{v}$.
    \end{enumerate}

    \tcblower
    \demostracion Se proponen como ejercicio.
\end{prop}

Nuestro primer ejemplo es el más simple de todos los espacios vectoriales, ya que contiene solo un objeto. Dado que el axioma 3 requiere que todo espacio vectorial contenga un vector cero, dicho objeto deberá ser precisamente ese vector.

\begin{examplebox}{}{}
    Sea $V$ un conjunto que consta de un único elemento, $\mathbf{0}$, y definimos
    $$\mathbf{0} + \mathbf{0} = \mathbf{0} \quad \text{ y } \quad \alpha \cdot \mathbf{0} = \mathbf{0}$$
    para todo escalar $\alpha$. A esto lo llamamos el \emph{espacio vectorial cero}. Este conjunto es particularmente interesante porque representa el caso más sencillo de un espacio vectorial, donde todas las operaciones de suma y multiplicación escalar son trivialmente definidas.
\end{examplebox}

Nuestro segundo ejemplo es uno de los más importantes de todos los espacios vectoriales: el familiar espacio $\RR[n]$. No debería sorprender que las operaciones en $\RR[n]$ satisfagan los axiomas de espacio vectorial, ya que dichos axiomas se basan en propiedades conocidas de las operaciones en $\RR$.

\begin{examplebox}{}{}
    Sea $V = \RR[n]$ y definamos las operaciones de espacio vectorial en $V$ como las operaciones usuales de suma y multiplicación escalar de $n$-tuplas; es decir, si
    $$\mathbf{u} = (u_1, u_2, \dots, u_n) \quad \text{ y } \quad \mathbf{v} = (v_1, v_2, \dots, v_n)$$
    entonces
    $$\mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n)$$
    y para $\alpha \in \RR$,
    $$\alpha \cdot \mathbf{u} = (\alpha u_1, \alpha u_2, \dots, \alpha u_n)$$
    El conjunto $V = \RR[n]$ está cerrado bajo la suma y la multiplicación escalar porque las operaciones anteriores producen $n$-tuplas como resultado final, y estas operaciones satisfacen los siguientes axiomas:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$, $\mathbf{v} = (v_1, v_2, \dots, v_n)$,
        \begin{align*}
            \mathbf{u} + \mathbf{v} & = (u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n) \\
            & = (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n) \\
            & = (v_1 + u_1, v_2 + u_2, \dots, v_n + u_n) \\
            & = (v_1, v_2, \dots, v_n) + (u_1, u_2, \dots, u_n) \\
            & = \mathbf{v} + \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de conmutatividad.
        \item Se deja como ejercicio al lector.
        \item Sea $\mathbf{u}$, $\mathbf{0} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$, $\mathbf{0} = (0, 0, \dots, 0)$,
        \begin{align*}
            \mathbf{u} + \mathbf{0} & = (u_1, u_2, \dots, u_n) + (0, 0, \dots, 0) \\
            & = (u_1 + 0, u_2 + 0, \dots, u_n + 0) \\
            & = (u_1, u_2, \dots, u_n) \\
            & = \mathbf{u}
        \end{align*}
        %\newpage
        Por tanto, se cumple la propiedad de neutro aditivo.
        \newpage
        \item Dado $\mathbf{u} = (u_1, u_2, \dots, u_n)$, existe $-\mathbf{u} = (-u_1, -u_2, \dots, -u_n)$ que cumple
        \begin{align*}
            \mathbf{u} + (-\mathbf{u}) & = (u_1, u_2, \dots, u_n) + (-u_1, -u_2, \dots, -u_n) \\
            & = \big(u_1 + (-u_1), u_2 + (-u_2), \dots, u_n + (-u_n) \big) \\
            & = (u_1 - u_1, u_2 - u_2, \dots, u_n - u_n) \\
            & = (0, 0, \dots, 0) \\
            & = \mathbf{0}
        \end{align*}
        Por tanto, se cumple la propiedad de inverso aditivo.
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$, $\mathbf{v} = (v_1, v_2, \dots, v_n)$ y sea $\alpha \in \RR$,
        \begin{align*}
            \alpha \cdot (\mathbf{u} + \mathbf{v}) & = \alpha \cdot [(u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n)] \\
            & = \alpha \cdot (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n) \\
            & = \big( \alpha(u_1 + v_1), \alpha(u_2 + v_2), \dots, \alpha(u_n + v_n) \big) \\
            & = (\alpha u_1 + \alpha v_1, \alpha u_2 + \alpha v_2, \dots, \alpha u_n + \alpha u_n) \\
            & = (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\alpha v_1, \alpha v_2, \dots, \alpha v_n) \\
            & = \alpha \cdot (u_1, u_2, \dots, u_n) + \alpha \cdot (v_1, v_2, \dots, v_n) \\
            & = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de vectores.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            (\alpha + \beta) \cdot \mathbf{u} & = (\alpha + \beta) \cdot (u_1, u_2, \dots, u_n) \\
            & = \big( (\alpha + \beta)u_1, (\alpha + \beta)u_2, \dots, (\alpha + \beta)u_n \big) \\
            & = (\alpha u_1 + \beta u_1, \alpha u_2 + \beta u_2, \dots, \alpha u_n + \beta u_n) \\
            & = (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\beta u_1, \beta u_2, \dots, \beta u_n) \\
            & = \alpha \cdot (u_1, u_2, \dots, u_n) + \beta \cdot (u_1, u_2, \dots, u_n) \\
            & = \alpha \cdot \mathbf{u} + \beta \cdot \mathbf{u}
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de escalares.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            \alpha \cdot (\beta \cdot \mathbf{u}) & = \alpha \cdot \big(\beta \cdot (u_1, u_2, \dots, u_n) \big) \\
            & = \alpha \cdot (\beta u_1, \beta u_2, \dots, \beta u_n) \\
            & = \big( \alpha (\beta u_1), \alpha (\beta u_2), \dots, \alpha (\beta u_n) \big) \\
            & = \big( (\alpha\beta) u_1, (\alpha\beta) u_2, \dots, (\alpha\beta) u_n \big) \\
            & = (\alpha\beta) \cdot (u_1, u_2, \dots, u_n) \\
            & = (\alpha\beta) \cdot \mathbf{u}
        \end{align*}
        Por tanto, se cumple la asociatividad sobre el producto escalar.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n)$ y sea $1 \in \RR$,
        \begin{align*}
            1 \cdot \mathbf{u} & = 1 \cdot (u_1, u_2, \dots, u_n) \\
            & = (1 u_1, 1 u_2, \dots, 1 u_n) \\
            & = (u_1, u_2, \dots, u_n) \\
            & = \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de identidad multiplicativa.
    \end{enumerate}
    Dado que hemos verificado que $\RR[n]$ satisface todas las propiedades requeridas para ser un espacio vectorial, concluimos que $\RR[n]$ es un espacio vectorial sobre $\RR$.
\end{examplebox}

Nuestro siguiente ejemplo es una generalización de $\RR[n]$ en la que permitimos que los vectores tengan infinitas componentes.

\newpage

\begin{examplebox}{}{}
    Sea $V$ el conjunto de objetos de la forma
    $$\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$$
    donde $u_1, u_2, \dots, u_n, \dots$ es una sucesión infinita de números reales. Definimos que dos sucesiones infinitas son \emph{iguales} si sus componentes correspondientes son iguales, y definimos la suma y la multiplicación escalar como
    $$\mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n, \dots)$$
    y para $\alpha \in \RR$,
    $$\alpha \cdot \mathbf{u} = (\alpha u_1, \alpha u_2, \dots, \alpha u_n, \dots).$$
    Denotaremos este espacio vectorial con el símbolo $\RR[\infty]$. El conjunto $V = \RR[\infty]$ está cerrado bajo la suma y la multiplicación escalar porque las operaciones anteriores producen sucesiones infinitas de números reales como resultado final, y estas operaciones satisfacen los siguientes axiomas:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$, $\mathbf{v} = (v_1, v_2, \dots, v_n, \dots)$,
        \begin{align*}
            \mathbf{u} + \mathbf{v} & = (u_1, u_2, \dots, u_n, \dots) + (v_1, v_2, \dots, v_n, \dots) \\
            & = (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n, \dots) \\
            & = (v_1 + u_1, v_2 + u_2, \dots, v_n + u_n, \dots) \\
            & = (v_1, v_2, \dots, v_n, \dots) + (u_1, u_2, \dots, u_n, \dots) \\
            & = \mathbf{v} + \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de conmutatividad.
        \item Se deja como ejercicio al lector.
        \item Sea $\mathbf{u}$, $\mathbf{0} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$, $\mathbf{0} = (0, 0, \dots, 0, \dots)$,
        \begin{align*}
            \mathbf{u} + \mathbf{0} & = (u_1, u_2, \dots, u_n, \dots) + (0, 0, \dots, 0, \dots) \\
            & = (u_1 + 0, u_2 + 0, \dots, u_n + 0, \dots) \\
            & = (u_1, u_2, \dots, u_n, \dots) \\
            & = \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de neutro aditivo.
        \item Dado $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$, existe $-\mathbf{u} = (-u_1, -u_2, \dots, -u_n, \dots)$ que cumple
        \begin{align*}
            \mathbf{u} + (-\mathbf{u}) & = (u_1, u_2, \dots, u_n, \dots) + (-u_1, -u_2, \dots, -u_n, \dots) \\
            & = \big(u_1 + (-u_1), u_2 + (-u_2), \dots, u_n + (-u_n), \dots \big) \\
            & = (u_1 - u_1, u_2 - u_2, \dots, u_n - u_n, \dots) \\
            & = (0, 0, \dots, 0, \dots) \\
            & = \mathbf{0}
        \end{align*}
        Por tanto, se cumple la propiedad de inverso aditivo.
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$, $\mathbf{v} = (v_1, v_2, \dots, v_n, \dots)$ y sea $\alpha \in \RR$,
        \begin{align*}
            \alpha \cdot (\mathbf{u} + \mathbf{v}) & = \alpha \cdot [(u_1, u_2, \dots, u_n, \dots) + (v_1, v_2, \dots, v_n, \dots)] \\
            & = \alpha \cdot (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n, \dots) \\
            & = \big( \alpha(u_1 + v_1), \alpha(u_2 + v_2), \dots, \alpha(u_n + v_n), \dots \big) \\
            & = (\alpha u_1 + \alpha v_1, \alpha u_2 + \alpha v_2, \dots, \alpha u_n + \alpha u_n, \dots) \\
            & = (\alpha u_1, \alpha u_2, \dots, \alpha u_n, \dots) + (\alpha v_1, \alpha v_2, \dots, \alpha v_n, \dots) \\
            & = \alpha \cdot (u_1, u_2, \dots, u_n, \dots) + \alpha \cdot (v_1, v_2, \dots, v_n, \dots) \\
            & = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}
        \end{align*}
        \newpage
        Por tanto, se cumple la distributividad sobre la suma de vectores.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            (\alpha + \beta) \cdot \mathbf{u} & = (\alpha + \beta) \cdot (u_1, u_2, \dots, u_n, \dots) \\
            & = \big( (\alpha + \beta)u_1, (\alpha + \beta)u_2, \dots, (\alpha + \beta)u_n, \dots \big) \\
            & = (\alpha u_1 + \beta u_1, \alpha u_2 + \beta u_2, \dots, \alpha u_n + \beta u_n, \dots) \\
            & = (\alpha u_1, \alpha u_2, \dots, \alpha u_n, \dots) + (\beta u_1, \beta u_2, \dots, \beta u_n, \dots) \\
            & = \alpha \cdot (u_1, u_2, \dots, u_n, \dots) + \beta \cdot (u_1, u_2, \dots, u_n, \dots) \\
            & = \alpha \cdot \mathbf{u} + \beta \cdot \mathbf{u}
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de escalares.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            \alpha \cdot (\beta \cdot \mathbf{u}) & = \alpha \cdot \big(\beta \cdot (u_1, u_2, \dots, u_n, \dots) \big) \\
            & = \alpha \cdot (\beta u_1, \beta u_2, \dots, \beta u_n, \dots) \\
            & = \big( \alpha (\beta u_1), \alpha (\beta u_2), \dots, \alpha (\beta u_n), \dots \big) \\
            & = \big( (\alpha\beta) u_1, (\alpha\beta) u_2, \dots, (\alpha\beta) u_n, \dots \big) \\
            & = (\alpha\beta) \cdot (u_1, u_2, \dots, u_n, \dots) \\
            & = (\alpha\beta) \cdot \mathbf{u}
        \end{align*}
        Por tanto, se cumple la asociatividad sobre el producto escalar.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = (u_1, u_2, \dots, u_n, \dots)$ y sea $1 \in \RR$,
        \begin{align*}
            1 \cdot \mathbf{u} & = 1 \cdot (u_1, u_2, \dots, u_n, \dots) \\
            & = (1 u_1, 1 u_2, \dots, 1 u_n, \dots) \\
            & = (u_1, u_2, \dots, u_n, \dots) \\
            & = \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de identidad multiplicativa.
    \end{enumerate}
    Dado que hemos verificado que $\RR[\infty]$ satisface todas las propiedades requeridas para ser un espacio vectorial, concluimos que $\RR[\infty]$ es un espacio vectorial sobre $\RR$.
\end{examplebox}

\sideFigure[\label{fig:voltajet}]{
\vspace{-4.5cm}\begin{tikzpicture}
    \draw[gray, ultra thick, domain=0.1:4.25, samples=20] plot[ycomb] (\x,{cos(1.745*\x r)});
    \draw[cw0, ultra thick, domain=0.1:4.25, samples=20] plot[only marks, mark=*, mark options={scale=0.7}] (\x,{cos(1.745*\x r)});
    \draw[-Stealth,thick] (0,-1.5) -- (0,2.25) node[below right] {\makecell[l]{$E(t)$ \\ Voltaje}};
    \draw[-Stealth,thick] (-0.5,0) -- (4.5,0) node[below left] {Tiempo};
    \node[above] at (4.5,0) {$t$};
    \draw (0,1) -- (-0.2,1) node[left] {$1$};
    \draw (0,-1) -- (-0.2,-1) node[left] {$-1$};
\end{tikzpicture}
}
Los espacios vectoriales del tipo en el ejemplo anterior surgen cuando una señal transmitida de duración indefinida es digitalizada al muestrear sus valores en intervalos de tiempo discretos (figura \ref{fig:voltajet}).

En el siguiente ejemplo, nuestros vectores serán matrices. Esto puede ser algo confuso al inicio, ya que las matrices están formadas por filas y columnas, que también son vectores. Sin embargo, desde la perspectiva del espacio vectorial, nos enfocamos en las propiedades de las operaciones matriciales en relación con la matriz en su totalidad, no en sus filas y columnas individuales.

\begin{examplebox}{}{}
    Sea $V$ el conjunto de matrices $2 \times 2$ con entradas reales, y consideremos en $V$ las operaciones de espacio vectorial dadas por la suma usual de matrices y la multiplicación por un escalar; es decir,
    \begin{equation}
        \mathbf{u} + \mathbf{v} = \begin{bmatrix}
            u_{11} & u_{12} \\
            u_{21} & u_{22}
        \end{bmatrix} + \begin{bmatrix}
            v_{11} & v_{12} \\
            v_{21} & v_{22}
        \end{bmatrix} = \begin{bmatrix}
            u_{11} + v_{11} & u_{12} + v_{12} \\
            u_{21} + v_{21} & u_{22} + v_{22}
        \end{bmatrix} \label{eq:suma_matriz22}
    \end{equation}
    y para $\alpha \in \RR$,
    $$\alpha \cdot \mathbf{u} = \alpha \cdot \begin{bmatrix}
        u_{11} & u_{12} \\
        u_{21} & u_{22}
    \end{bmatrix} = \begin{bmatrix}
        \alpha u_{11} & \alpha u_{12} \\
        \alpha u_{21} & \alpha u_{22}
    \end{bmatrix}.$$
    El conjunto $V$ es cerrado bajo la suma y el producto escalar, ya que estas operaciones producen matrices $2 \times 2$ como resultado final. Se deja como ejercicio al lector verificar que se cumplen los axiomas (i), (ii), (iii), (iv), (v), (vi), (vii), (vii).
\end{examplebox}
\infoBulle{Tenga en cuenta que la ecuación \eqref{eq:suma_matriz22} implica tres operaciones de suma diferentes: la operación de suma de vectores, la operación de suma de matrices y la operación de suma de números reales.}

\newpage

\begin{examplebox}{}{}
    El ejemplo anterior es un caso particular de una clase más general de espacios vectoriales. No debería ser difícil adaptar el argumento utilizado en ese ejemplo para demostrar que el conjunto $V$ de todas las matrices $m \times n$ con las operaciones usuales de suma y producto escalar es un espacio vectorial. Denotaremos este espacio vectorial con el símbolo $\mathcalm{M}_{m \times n}$. Así, por ejemplo, el espacio vectorial en el ejemplo anterior se denota como $\mathcalm{M}_{2 \times 2}$.
\end{examplebox}

\sideFigure[\label{fig:propfunc}]{
    \begin{flushleft}
        \begin{tikzpicture}
            \tikzmath{
                \c = 1.25;
                \a = \c - 1.5;
                \b = \c + 1.5;
                function f(\x) {
                    return -0.1*(\x-\c)^2 + 0.7;
			    };
                function g(\x) {
                    return 0.3*(\x-\c)^2 + 1.7;
			    };
                function h(\x) {
                    return f(\x) + g(\x);
			    };
		    };
            \draw[cw0,thick] plot[domain = \a:\b, samples = 120] (\x,{f(\x)});
            \draw[cw2,thick] plot[domain = \a:\b, samples = 120] (\x,{g(\x)});
            \draw[cw0!50,thick] plot[domain = \a:\b, samples = 120] (\x,{h(\x)});
            \draw (\c,0) -- (\c,-0.2) node[below] {$x$};
            %
            \draw[-Stealth,thick] (0,-0.5) -- (0,3.5) node[below right] {$y$};
            \draw[-Stealth,thick] (-1,0) -- (3.5,0) node[below left] {$x$};
            %
            \node[left] at (\a,{f(\a)}) {$\mathbf{f}$};
            \node[left] at (\a,{g(\a)}) {$\mathbf{g}$};
            \node[left] at (\a,{h(\a)}) {$\mathbf{f} + \mathbf{g}$};
            %
            \filldraw[cw0] (\c,{f(\c)}) circle (2pt);
            \filldraw[cw2] (\c,{g(\c)}) circle (2pt);
            \filldraw[cw0!50] (\c,{h(\c)}) circle (2pt);
            %
            \draw[black, decorate, decoration={brace,raise=5pt}, transform shape] (\c,0.05) -- node[midway, left, xshift=-7pt] {$f(x)$} (\c,{f(\c)});
            \draw[black, decorate, decoration={brace,mirror,raise=5pt}, transform shape] (\c,0.05) -- node[midway, right, xshift=7pt, yshift=3pt] {$g(x)$} (\c,{g(\c)});
            \draw[black, decorate, decoration={brace,mirror,raise=5pt}, transform shape] (2.1,0.05) -- node[midway, right, xshift=7pt] {$f(x) + g(x)$} (2.1,{h(\c)});
            \node at (0,5) {~};
        \end{tikzpicture}
        \TituloBox{\normalsize(a)}\\[3mm]
        \begin{tikzpicture}
            \tikzmath{
                \c = 1.25;
                \a = \c - 1.5;
                \b = \c + 1.5;
                function f(\x) {
                    return -0.1*(\x-\c)^2 + 0.7;
			    };
                function h(\x) {
                    return 2.5*f(\x);
			    };
		    };
            \draw[cw0,thick] plot[domain = \a:\b, samples = 120] (\x,{f(\x)});
            \draw[cw2,thick] plot[domain = \a:\b, samples = 120] (\x,{h(\x)});
            \draw (\c,0) -- (\c,-0.2) node[below] {$x$};
            %
            \draw[-Stealth,thick] (0,-0.5) -- (0,3.5) node[below right] {$y$};
            \draw[-Stealth,thick] (-1,0) -- (3.5,0) node[below left] {$x$};
            %
            \node[left] at (\a,{f(\a)}) {$\mathbf{f}$};
            \node[left] at (\a,{h(\a)}) {$\alpha \cdot \mathbf{f}$};
            %
            \filldraw[cw0] (\c,{f(\c)}) circle (2pt);
            \filldraw[cw0!50] (\c,{h(\c)}) circle (2pt);
            %
            \draw[black, decorate, decoration={brace,raise=5pt}, transform shape] (\c,0.05) -- node[midway, left, xshift=-7pt] {$f(x)$} (\c,{f(\c)});
            \draw[black, decorate, decoration={brace,mirror,raise=5pt}, transform shape] (\c,0.05) -- node[midway, right, xshift=7pt, yshift=3pt] {$\alpha f(x)$} (\c,{h(\c)});
        \end{tikzpicture}
        \,\\
        \TituloBox{\normalsize(b)}\\[3mm]
        \begin{tikzpicture}
            \tikzmath{
                \c = 1.25;
                \a = \c - 1.5;
                \b = \c + 1.5;
                function f(\x) {
                    return -0.1*(\x-\c)^2 + 0.7;
			    };
                function h(\x) {
                    return -1*f(\x);
			    };
		    };
            \draw[cw0,thick] plot[domain = \a:\b, samples = 120] (\x,{f(\x)});
            \draw[cw2,thick] plot[domain = \a:\b, samples = 120] (\x,{h(\x)});
            \draw (\c,0) -- (\c,-0.2);
            %
            \draw[-Stealth,thick] (0,-1.5) -- (0,2.5) node[below right] {$y$};
            \draw[cw0!50,thick] (\a,0) -- (3.4,0);
            \draw[-Stealth,thick] (3.45,0) -- (3.5,0) node[below left] {$x$};
            \draw[white] (-1,0) -- (-0.99,0);
            %
            \node[left] at (\a,{f(\a)}) {$\mathbf{f}$};
            \node[left] at (\a,{h(\a)}) {$-\mathbf{f}$};
            \node[left] at (\a,0) {$\mathbf{0}$};
            %
            \filldraw[cw0] (\c,{f(\c)}) circle (2pt);
            \filldraw[cw0!50] (\c,{h(\c)}) circle (2pt);
            %
            \draw[black, decorate, decoration={brace,raise=5pt}, transform shape] (\c,0.05) -- node[midway, left, xshift=-7pt] {$f(x)$} (\c,{f(\c)});
            \draw[black, decorate, decoration={brace,raise=5pt}, transform shape] (\c,{h(\c)}) -- node[midway, left, xshift=-7pt] {$-f(x)$} (\c,-0.05);
        \end{tikzpicture}
        \,\\
        \TituloBox{\normalsize(c)}
    \end{flushleft}
}

\begin{examplebox}{}{}
    Sea $V$ el conjunto de funciones de valor real que están definidas en cada $x$ en el intervalo $(-\infty, \infty)$. Si $\mathbf{f} = f(x)$ y $\mathbf{g} = g(x)$ son dos funciones en $V$ y si $\alpha$ es un escalar cualquiera, entonces definimos las operaciones de suma y producto escalar como
    \begin{equation}
        (\mathbf{f} + \mathbf{g})(x) = f(x) + g(x) \label{eq:sumafuncii}
    \end{equation}
    y para $\alpha$,
    \begin{equation}
        (\alpha \cdot \mathbf{f})(x) = \alpha f(x). \label{eq:prodfuncii}
    \end{equation}
    Una forma de interpretar estas operaciones es ver los valores $f(x)$ y $g(x)$ como las “componentes” de $\mathbf{f}$ y $\mathbf{g}$ en el punto $x$. En este caso, las ecuaciones \eqref{eq:sumafuncii} y \eqref{eq:prodfuncii} establecen que dos funciones se suman agregando sus componentes correspondientes, y que una función se multiplica por un escalar multiplicando cada componente por dicho escalar, exactamente como en $\RR[n]$ y $\RR[\infty]$. Esta idea se ilustra en las partes (a) y (b) de la figura \ref{fig:propfunc}. El conjunto $V$ con estas operaciones se denota por el símbolo $F(-\infty, \infty)$. Podemos demostrar que este es un espacio vectorial de la siguiente manera:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Sea $\mathbf{f}$, $\mathbf{g} \in V$ con $\mathbf{f} = f(x)$, $\mathbf{g} = g(x)$,
        \begin{align*}
            (\mathbf{f} + \mathbf{g})(x) & = f(x) + g(x) \\
            & = g(x) + f(x) \\
            & = (\mathbf{g} + \mathbf{f})(x)
        \end{align*}
        Por tanto, se cumple la propiedad de conmutatividad.
        \item Sea $\mathbf{f}$, $\mathbf{g}$, $\mathbf{h} \in V$ con $\mathbf{f} = f(x)$, $\mathbf{g} = g(x)$, $\mathbf{h} = h(x)$,
        \begin{align*}
            [\mathbf{f} + (\mathbf{g} + \mathbf{h})](x) & = f(x) + (g(x) + h(x)) \\
            & = (f(x) + g(x)) + h(x) \\
            & = [(\mathbf{f} + \mathbf{g}) + \mathbf{h}](x)
        \end{align*}
        Por tanto, se cumple la asociatividad sobre la suma de vectores.
        \item Existe una función $\mathbf{0}$ en $F(-\infty, \infty)$, la cual, al sumarse con cualquier otra función $\mathbf{f}$ en $F(-\infty, \infty)$, devuelve $\mathbf{f}$ como resultado. La función cuyo valor en cada punto $x$ del intervalo $(-\infty, \infty)$ es cero cumple con esta propiedad. Geométricamente, el gráfico de la función $\mathbf{0}$ es la línea que coincide con el eje $x$. De esta forma, $\mathbf{f}$, $\mathbf{0} \in V$ con $\mathbf{f} = f(x)$, $\mathbf{0} = 0$,
        \begin{align*}
            (\mathbf{f} + \mathbf{0})(x) & = f(x) + 0 \\
            & = f(x) \\
            & = \mathbf{f}
        \end{align*}
        Por tanto, se cumple la propiedad de neutro aditivo.
        \item Para cada función $\mathbf{f}$ en $F(-\infty, \infty)$ existe una función $-\mathbf{f}$ en $F(-\infty, \infty)$, la cual, al sumarse con $\mathbf{f}$, produce la función $\mathbf{0}$. La función definida por $-\mathbf{f}(x) = -f(x)$ cumple con esta propiedad. Geométricamente, el gráfico de $-\mathbf{f}$ se obtiene reflejando el de $\mathbf{f}$ con respecto al eje $x$ (figura \ref{fig:propfunc}c). Así,
        \begin{align*}
            [\mathbf{f} + (-\mathbf{f})](x) & = f(x) + (-f(x)) \\
            & = f(x) - f(x) \\
            & = 0 \\
            & = \mathbf{0}
        \end{align*}
        Por tanto, se cumple la propiedad de inverso aditivo.
        \item Sea $\mathbf{f}$, $\mathbf{g} \in V$ con $\mathbf{f} = f(x)$, $\mathbf{g} = g(x)$ y sea $\alpha \in \RR$,
        \begin{align*}
            [\alpha \cdot (\mathbf{f} + \mathbf{g})](x) & = \alpha (f(x) + g(x)) \\
            & = \alpha f(x) + \alpha g(x) \\
            & = (\alpha \cdot \mathbf{f})(x) + (\alpha \cdot \mathbf{g})(x) \\
            & = (\alpha \cdot \mathbf{f} + \alpha \cdot \mathbf{g})(x)
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de vectores.
        \item Sea $\mathbf{f} \in V$ con $\mathbf{f} = f(x)$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            [(\alpha + \beta) \cdot \mathbf{f}](x) & = (\alpha + \beta) f(x) \\
            & = \alpha f(x) + \beta f(x) \\
            & = (\alpha \cdot \mathbf{f})(x) + (\beta \cdot \mathbf{f})(x) \\
            & = (\alpha \cdot \mathbf{f} + \beta \cdot \mathbf{f})(x)
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de escalares.
        \item Se deja como ejercicio al lector.
        \item Sea $\mathbf{f} \in V$ con $\mathbf{f} = f(x)$ y sea $1 \in \RR$,
        \begin{align*}
            (1 \cdot \mathbf{f})(x) & = 1 f(x) \\
            & = f(x) \\
            & = \mathbf{f}
        \end{align*}
        Por tanto, se cumple la propiedad de identidad multiplicativa.
    \end{enumerate}
    Dado que hemos verificado que $F(-\infty, \infty)$ satisface todas las propiedades requeridas para ser un espacio vectorial, concluimos que $F(-\infty, \infty)$ es un espacio vectorial sobre $\RR$.
\end{examplebox}

Es importante reconocer que no se pueden imponer arbitrariamente dos operaciones en un conjunto $V$ y esperar que se cumplan los axiomas de espacio vectorial. Por ejemplo, si $V$ es el conjunto de $n$-tuplas con componentes \emph{positivas}, y si se utilizan las operaciones estándar de $\RR[n]$, entonces $V$ no es cerrado bajo la multiplicación por escalares. Esto se debe a que, si $\mathbf{u}$ es una $n$-tupla no nula en $V$, entonces $(-1) \cdot \mathbf{u}$ tiene al menos un componente negativo y, por lo tanto, no pertenece a $V$.

El siguiente es un ejemplo menos evidente en el que solo uno de los ocho axiomas de espacio vectorial deja de cumplirse.

\begin{examplebox}{}{}
    Sea $V = \RR[2]$ y definamos las operaciones de suma y producto escalar en $V$ de la siguiente manera: Si $\mathbf{u} = (u_1, u_2)$, $\mathbf{v} = (v_1, v_2)$ y si $\alpha$ es un número real cualquiera, definimos
    $$\mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2) \quad \text{ y } \quad \alpha \cdot \mathbf{u} = (\alpha u_1, 0).$$
    Por ejemplo, si $\mathbf{u} = (2, 4)$, $\mathbf{v} = (-3, 5)$ y $\alpha = 7$, entonces
    $$\mathbf{u} + \mathbf{v} = \big(2 + (-3), 4 + 5\big) = (-1, 9)$$
    y
    $$\alpha \cdot \mathbf{u} = 7 \cdot \mathbf{u} = (7 \cdot 2, 0) = (14, 0).$$
    La operación de suma es la estándar en $\RR[2]$, pero el producto escalar no lo es. Los primeros siete axiomas de espacio vectorial se cumplen, sin embargo, el axioma (viii) no se cumple para ciertos vectores. Por ejemplo, si $\mathbf{u} = (u_1, u_2)$ es tal que $u_2 \neq 0$, entonces
    $$1 \cdot \mathbf{u} = 1 \cdot (u_1, u_2) = (1 \cdot u_1, 0) = (u_1, 0) \neq \mathbf{u}.$$
    Por lo tanto, $V$ no es un espacio vectorial con las operaciones establecidas.
\end{examplebox}

\newpage

Nuestro último ejemplo será un espacio vectorial inusual que hemos incluido para ilustrar la variedad de espacios vectoriales.

\begin{examplebox}{}{}
    Sea $V$ el conjunto de los números reales positivos, y sean $\mathbf{u} = u$ y $\mathbf{v} = v$ cualesquiera vectores (es decir, números reales positivos) en $V$. Sea $\alpha$ un escalar cualquiera. Definimos las operaciones en $V$ como
    $$\mathbf{u} + \mathbf{v} = uv \quad \text{ y } \quad \alpha \cdot \mathbf{u} = u^{\alpha}.$$
    Por ejemplo, $1 + 1 = 1$ y $(2)(1) = 1^2 = 1$, lo cual resulta extraño, pero, no obstante, el conjunto $V$ con estas operaciones satisface los ocho axiomas de los espacios vectoriales y, por lo tanto, es un espacio vectorial. Verificaremos algunos axiomas, y dejaremos los demás como ejercicio al lector.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = u$, $\mathbf{v} = v$,
        \begin{align*}
            \mathbf{u} + \mathbf{v} & = uv \\
            & = vu \\
            & = \mathbf{v} + \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de conmutatividad.
        \item Se deja como ejercicio al lector.
        \item Sea $\mathbf{u}$, $\mathbf{0} \in V$ con $\mathbf{u} = u$, $\mathbf{0} = 1$,
        \begin{align*}
            \mathbf{u} + \mathbf{0} & = u^1 \\
            & = u \\
            & = \mathbf{u}
        \end{align*}
        Por tanto, se cumple la propiedad de neutro aditivo.
        \item Dado $\mathbf{u} = u$, existe $-\mathbf{u} = \dfrac{1}{u}$ que cumple
        \begin{align*}
            \mathbf{u} + (-\mathbf{u}) & = u \left( \frac{1}{u} \right) \\
            & = 1 \\
            & = \mathbf{0}
        \end{align*}
        Por tanto, se cumple la propiedad de inverso aditivo.
        \item Sea $\mathbf{u}$, $\mathbf{v} \in V$ con $\mathbf{u} = u$, $\mathbf{v} = v$ y sea $\alpha \in \RR$,
        \begin{align*}
            \alpha \cdot (\mathbf{u} + \mathbf{v}) & = (uv)^{\alpha} \\
            & = u^{\alpha} v^{\alpha} \\
            & = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de vectores.
        \item Sea $\mathbf{u} \in V$ con $\mathbf{u} = u$ y sea $\alpha$, $\beta \in \RR$,
        \begin{align*}
            (\alpha + \beta) \cdot \mathbf{u} & = u^{\alpha + \beta} \\
            & = u^{\alpha} u^{\beta} \\
            & = \alpha \cdot \mathbf{u} + \beta \cdot \mathbf{u}
        \end{align*}
        Por tanto, se cumple la distributividad sobre la suma de escalares.
        \item Se deja como ejercicio al lector.
        \item Se deja como ejercicio al lector.
    \end{enumerate}
    En conclusión, aunque estas operaciones resulten inusuales, la verificación de los ocho axiomas, confirma que esta estructura cumple con todos los requisitos para ser considerada un espacio vectorial sobre $\RR$.
\end{examplebox}
\infoBulle{Este caso resalta la generalidad del concepto de espacio vectorial, mostrando que la validez de una estructura vectorial depende de la satisfacción de sus axiomas, más que de la forma convencional de sus operaciones.}

\newpage

\subsection{Subespacios vectoriales}

A menudo ocurre que algún espacio vectorial de interés está contenido dentro de un espacio vectorial más grande cuyas propiedades son conocidas. En esta sección mostraremos cómo reconocer cuándo ocurre esto, explicaremos cómo las propiedades del espacio vectorial más grande pueden usarse para obtener propiedades del espacio vectorial más pequeño, y proporcionaremos una variedad de ejemplos importantes.

\begin{definicion}{}{}
    Se dice que $W$ es un subespacio vectorial de $V$ si $W$ es un subconjunto no vacío de $V$, y $W$ es un espacio vectorial, junto con las operaciones de suma entre vectores y producto escalar definidas para $V$.
\end{definicion}

En general, para demostrar que un conjunto no vacío $W$ con dos operaciones es un espacio vectorial, se deben verificar los diez axiomas de espacio vectorial. Sin embargo, si $W$ es un subespacio de un espacio vectorial conocido $V$, entonces ciertos axiomas no necesitan ser verificados porque son “heredados” de $V$. Por ejemplo, no es necesario verificar que $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ se cumple en $W$ porque se cumple para todos los vectores en $V$, incluidos los de $W$. Por otro lado, es necesario verificar que $W$ está cerrado bajo la suma y el producto escalar, ya que es posible que sumar dos vectores en $W$ o multiplicar un vector en $W$ por un escalar produzca un vector en $V$ que no esté en $W$ (figura \ref{JAJJAJAJAJJQJQOOQPQZ}).
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \coordinate (A) at (0,2);
        \coordinate (B) at (5,0);
        \coordinate (C) at (7,1.5);
        \coordinate (D) at (2,3.5);
        %
        \draw (A) -- (B) node[above] {$V$} -- (C) -- (D) -- cycle;
        \filldraw[cw1] ($(C)!.78!(A)$) -- ($(D)!.78!(B)$) -- ($(A)!.78!(C)$) -- ($(B)!.78!(D)$) -- cycle;
        \draw ($(C)!.78!(A)$) -- ($(D)!.78!(B)$) node[above] {$W$} -- ($(A)!.78!(C)$) -- ($(B)!.78!(D)$) -- cycle;
        \filldraw (3.25,1.5) circle (1.5pt);
        \draw[-latex] (3.25,1.5) -- node[midway,below] {$\mathbf{v}$} (4,1.75);
        \draw[-latex] (3.25,1.5) -- node[midway,left] {$\mathbf{u}$} (2.8,2.4);
        \draw[-latex] (3.25,1.5) -- (3.55,2.65);
        \draw[dashed] (2.8,2.4) -- (3.55,2.65) -- (4,1.75);
        \draw[-latex] (2.8,2.4) -- node[midway,left] {$\alpha \cdot \mathbf{u}$} (2.4,3.2);
        \draw[thin] (3.5,2) -- (4.5,2.75) node[above] {$\mathbf{u} + \mathbf{v}$};
    \end{tikzpicture}
    \caption{Los vectores $\mathbf{u}$ y $\mathbf{v}$ están en $W$, pero los vectores $\mathbf{u} + \mathbf{v}$ y $\alpha \mathbf{u}$ no lo están}
    \label{JAJJAJAJAJJQJQOOQPQZ}
\end{figure}
\noindent Los axiomas que no son heredados por $W$ son:
\begin{enumerate}
    \item[iii)] Existencia de un vector cero en $W$.
    \item[iv)] Existencia de un negativo en $W$ para cada vector en $W$.
\end{enumerate}
Por lo tanto, estos deben ser verificados para probar que $W$ es un subespacio de $V$. Sin embargo, el siguiente teorema muestra que si los $W$ está cerrado bajo la suma y el producto escalar, entonces los axiomas (iii) y (iv) se cumplen en $W$ como consecuencia y, por lo tanto, no necesitan ser verificados.

\begin{theorem}{}{}
    Si $W$ es un conjunto de uno o más vectores en un espacio vectorial $V$, entonces $W$ es un subespacio de $V$ si y solo si se satisfacen las siguientes condiciones:
    \begin{enumerate}[label=\alph*), topsep=6pt, itemsep=0pt]
        \item Si $\mathbf{u}$ y $\mathbf{v}$ son vectores en $W$, entonces $\mathbf{u} + \mathbf{v}$ está en $W$.
        \item Si $\alpha$ es un escalar y $\mathbf{u}$ es un vector en $W$, entonces $\alpha \cdot \mathbf{u}$ está en $W$.
    \end{enumerate}

    \tcblower
    \demostracion Por definición, un subespacio es un espacio vectorial cuyos elementos pertenecen a $V$ y cuyas operaciones son las mismas que en $V$. Como $V$ es un espacio vectorial, se tiene que $W$ está cerrado bajo la suma y el producto escalar, que corresponden exactamente a las condiciones (a) y (b). Recíprocamente, supongamos que se satisfacen las condiciones (a) y (b). Dado que los axiomas (i), (ii), (v), (vi), (vii) y (viii) son heredados de $V$, solo necesitamos demostrar que los axiomas (iii) y (iv) se cumplen en $W$. Para ello, sea $\mathbf{u}$ un vector cualquiera en $W$. De la condición (b) se sigue que $\alpha \cdot \mathbf{u}$ es un vector en $W$ para cualquier escalar $\alpha$. En particular, $0 \cdot \mathbf{u} = \mathbf{0}$ y $(-1) \cdot \mathbf{u} = -\mathbf{u}$ están en $W$, lo que muestra que los axiomas (iii) y (iv) se cumplen en $W$. 
\end{theorem}

\newpage

\begin{examplebox}{}{}
    Si $V$ es un espacio vectorial y $W = \{ \mathbf{0} \}$ es el subconjunto de $V$ que consiste únicamente en el vector cero, entonces $W$ está cerrado bajo la suma y la multiplicación por un escalar, ya que
    $$\mathbf{0} + \mathbf{0} = \mathbf{0} \quad \text{ y } \quad \alpha \cdot \mathbf{0} = \mathbf{0}$$
    para cualquier escalar $\alpha$. A $W$ lo llamamos el \emph{subespacio cero} de $V$.
\end{examplebox}

\begin{examplebox}{}{}
    Si $W$ es una recta que pasa por el origen en $\RR[2]$ o $\RR[3]$, entonces la suma de dos vectores en la recta o la multiplicación de un vector en la recta por un escalar produce otro vector en la recta. Por lo tanto, $W$ está cerrado bajo la suma y el producto escalar (véase la figura \ref{fig:rectaEV} para una ilustración en $\RR[3]$).
    \begin{center}
        \begin{tikzpicture}
            \draw[thick] (-0.5,-0.25) -- (3.75,1.875) node[right] {$W$};
            \draw[-latex,draw={cw0},thick] (0,0) -- (1,0.5) node[above left] {$\mathbf{u}$};
            \draw[-latex,draw={cw0},thick] (1,0.5) -- (2,1) node[above left] {$\mathbf{v}$};
            \draw[-latex,draw={cw0},thick] (2,1) -- (3,1.5) node[above left] {$\mathbf{u} + \mathbf{v}$};
            \draw[-Stealth,thick] (0,0) -- (0,3) node[below right] {$y$};
            \draw[-Stealth,thick] (0,0) -- (4.2,0) node[below left] {$x$};
            \draw[-Stealth,thick] (0,0) -- (-0.65,-0.9) node[above left] {$z$};
        \end{tikzpicture}
        \hfill
        \begin{tikzpicture}
            \draw[thick] (-0.5,-0.25) -- (3.75,1.875) node[right] {$W$};
            \draw[-latex,draw={cw0},thick] (0,0) -- (1,0.5) node[above left] {$\mathbf{u}$};
            \draw[-latex,draw={cw0},thick] (1,0.5) -- (2.5,1.25) node[above left] {$\alpha \cdot \mathbf{u}$};
            \draw[-Stealth,thick] (0,0) -- (0,3) node[below right] {$y$};
            \draw[-Stealth,thick] (0,0) -- (4.2,0) node[below left] {$x$};
            \draw[-Stealth,thick] (0,0) -- (-0.65,-0.9) node[above left] {$z$};
        \end{tikzpicture}
        
        \TituloBox{(a)} $W$ es cerrado bajo la suma \hfill \TituloBox{(b)} $W$ es cerrado bajo el producto escalar\captionsetup*[figure]{hypcap=false}
        \captionof{figure}{Los vectores $\mathbf{u} + \mathbf{v}$ y $\alpha \cdot \mathbf{u}$ se encuentran en la misma recta que $\mathbf{u}$ y $\mathbf{v}$}\label{fig:rectaEV}
    \end{center}
\end{examplebox}

\begin{examplebox}{}{}
    Si $\mathbf{u}$ y $\mathbf{v}$ son vectores en un plano $W$ que pasa por el origen en $\RR[3]$, entonces es evidente geométricamente que $\mathbf{u} + \mathbf{v}$ y $\alpha \cdot \mathbf{u}$ también pertenecen al mismo plano $W$ para cualquier escalar $\alpha$ (figura \ref{fig:planoEV}). Por lo tanto, $W$ está cerrado bajo la suma y el producto escalar.
    \begin{center}
        \begin{tikzpicture}
            \filldraw[cw2,opacity=0.1] (4.25,3) -- (0.75,3) -- (-0.25,-1) -- (3.25,-1) -- cycle;
            \draw[dashed] (1,2) -- (3,2.5) -- (2,0.5);
            \draw[-latex,draw={cw0},thick] (0,0) -- (2,0.5) node[below] {$\mathbf{u}$};
            \draw[-latex,draw={cw0},thick] (0,0) -- (3,0.75) node[below] {~~$\alpha \cdot \mathbf{u}$};
            \draw[-latex,draw={cw0},thick] (0,0) -- (1,2) node[above left] {$\mathbf{v}$};
            \draw[-latex,draw={cw0},thick] (0,0) -- (3,2.5) node[above left] {$\mathbf{u} + \mathbf{v}$};
            \draw[-Stealth,thick] (0,0) -- (0,3.5) node[below right] {$y$};
            \draw[-Stealth,thick] (0,0) -- (4.2,0) node[below left] {$x$};
            \draw[-Stealth,thick] (0,0) -- (-0.65,-0.9) node[above left] {$z$};
            \node[above left] at (3.25,-1) {$W$};
        \end{tikzpicture}
        \captionsetup*[figure]{hypcap=false}
        \captionof{figure}{Los vectores $\mathbf{u} + \mathbf{v}$ y $\alpha \cdot \mathbf{u}$ se encuentran en el mismo plano que $\mathbf{u}$ y $\mathbf{v}$}\label{fig:planoEV}
    \end{center}
\end{examplebox}

La tabla \ref{tab:subespR23} a continuación presenta una lista de los subespacios de $\RR[2]$ y $\RR[3]$ que hemos encontrado hasta ahora.
\begin{table}[H]
    \centering
    \begin{NiceTabular}{ll}[cell-space-limits=2pt]
        \CodeBefore
        \rowcolor{cw1}{1}
        \Body
        \toprule
        Subespacios de $\RR[2]$ & Subespacios de $\RR[3]$ \\
        \midrule
        $\bullet$\quad $\{ \mathbf{0} \}$ & $\bullet$\quad $\{ \mathbf{0} \}$ \\
        $\bullet$\quad Rectas que pasan por el origen & $\bullet$\quad Rectas que pasan por el origen \\
        $\bullet$\quad $\RR[2]$ & $\bullet$\quad Planos que pasan por el origen \\
        & $\bullet$\quad $\RR[3]$ \\
        \bottomrule
    \end{NiceTabular}
    \caption{Subespacios vectoriales de $\RR[2]$ y $\RR[3]$}
    \label{tab:subespR23}
\end{table}
\begin{examplebox}{}{}
    Sea $W$ el conjunto de todos los puntos $(x, y)$ en $\RR[2]$ para los cuales $x \geq 0$ y $y \geq 0$. Este conjunto no es subespacio de $\RR[2]$ porque no es cerrado bajo el producto escalar. Por ejemplo, $\mathbf{v} = (1, 1)$ es un vector en $W$, pero $(-1) \cdot \mathbf{v} = (-1, -1)$ no lo es.
\end{examplebox}

\newpage

\begin{examplebox}{}{}
    El conjunto $W$ de matrices invertibles de $n \times n$ no es un subespacio de $\mathcalm{M}_{n \times n}$, ya que falla en dos aspectos: no es cerrado bajo la suma ni bajo el producto escalar. Ilustraremos esto con un ejemplo en $\mathcalm{M}_{2 \times 2}$ que se puede adaptar fácilmente a $\mathcalm{M}_{n \times n}$. Consideremos las matrices
    $$U = \begin{bmatrix} 1 & 2 \\ 2 & 5 \end{bmatrix} \quad \text{ y } \quad V = \begin{bmatrix*}[r] -1 & 2 \\ -2 & 5 \end{bmatrix*}.$$
    La matriz $0 \cdot U$ es la matriz cero de $2 \times 2$, por lo que no es invertible, y la matriz $U + V$ tiene una columna de ceros, lo que implica que tampoco es invertible.
\end{examplebox}

\begin{examplebox}{}{EJEMPLOINI}
    Del cálculo, sabemos que una función $f$ es \emph{continua en $a$} si
    $$\lim_{x \to a} f(x) = f(a).$$
    Además, si $f$ y $g$ son continuas en $a$, entonces $f + g$ es continua; y si $\alpha$ es un escalar, entonces $\alpha f$ también es continua. En lenguaje vectorial, el conjunto de funciones continuas en $(-\infty, \infty)$ es un subespacio de $F(-\infty, \infty)$. Denotaremos este subespacio como $C(-\infty, \infty)$.
\end{examplebox}

\begin{examplebox}{}{}
    Una función con una derivada continua se dice que es \emph{continuamente diferenciable}. Existe un teorema en cálculo que establece que la suma de dos funciones continuamente diferenciables es continuamente diferenciable y que una constante multiplicada por una función continuamente diferenciable también es continuamente diferenciable. Por lo tanto, las funciones que son continuamente diferenciables en $(-\infty, \infty)$ forman un subespacio de $F(-\infty, \infty)$. Denotaremos este subespacio por $C^1(-\infty, \infty)$, donde el superíndice enfatiza que las primeras derivadas son continuas. Para llevar esto un paso más allá, el conjunto de funciones con $m$ derivadas continuas en $(-\infty, \infty)$ es un subespacio de $F(-\infty, \infty)$, al igual que el conjunto de funciones con derivadas de todos los órdenes en $(-\infty, \infty)$. Denotaremos estos subespacios por $C^m(-\infty, \infty)$ y $C^{\infty}(-\infty, \infty)$, respectivamente.
\end{examplebox}

\begin{examplebox}{}{}
    Recordemos que un \emph{polinomio} es una función que puede expresarse en la forma
    $$p(x) = a_0 + a_1x + \cdots + a_nx^n$$
    donde $a_0, a_1, \dots, a_n$ son constantes. Es evidente que la suma de dos polinomios es un polinomio y que una constante multiplicada por un polinomio también es un polinomio. Por lo tanto, el conjunto $W$ de todos los polinomios es un subespacio de $F(-\infty, \infty)$. Este espacio se denota por $P_{\infty}$.
\end{examplebox}

\begin{examplebox}{}{EJEMPLOFIN}
    Recordemos que el grado de un polinomio es la mayor potencia de la variable que aparece con un coeficiente distinto de cero. No es cierto que el conjunto $W$ de polinomios de grado exacto $n$ sea un subespacio de $F(-\infty, \infty)$ porque ese conjunto no está cerrado bajo la suma. Por ejemplo, los polinomios
    $$1 + 2x + 3x^2 \quad \text{ y } \quad 5 + 7x - 3x^2$$
    tienen grado $2$, pero su suma tiene grado $1$. Sin embargo, para cada entero no negativo $n$, el conjunto de polinomios de grado menor o igual a $n$ forma un subespacio de $F(-\infty, \infty)$, ya que está cerrado bajo la suma y el producto escalar. Denotaremos este espacio por $P_n$.
\end{examplebox}

En nuestros ejemplos anteriores consideramos funciones que estaban definidas en todos los puntos del intervalo $(-\infty, \infty)$. A veces querremos considerar funciones que están definidas únicamente en algún subintervalo de $(-\infty, \infty)$, como el intervalo cerrado $[a, b]$ o el intervalo abierto $(a, b)$. En tales casos, realizaremos un cambio de notación apropiado. Por ejemplo, $C[a, b]$ es el espacio de funciones continuas en $[a, b]$, y $C(a, b)$ es el espacio de funciones continuas en $(a, b)$.

\newpage

En cálculo se demuestra que los polinomios son funciones continuas y tienen derivadas continuas de todos los órdenes en $(-\infty, \infty)$. Por lo tanto, se deduce que $P_\infty$ no solo es un subespacio de $F(-\infty, \infty)$, como se observó anteriormente, sino que también es un subespacio de $C^\infty(-\infty, \infty)$. Dejamos al lector la tarea de convencerse de que los espacios vectoriales discutidos en los ejemplos \ref{examplebox:EJEMPLOINI} a \ref{examplebox:EJEMPLOFIN} están “anidados” unos dentro de otros, como se ilustra en la figura \ref{JAJAIQPAPOAOSOOAKJS}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}%[font=\small]
        \coordinate (A) at (0,0);
        \coordinate (B) at (10,0);
        \coordinate (C) at (11.5,6);
        \coordinate (D) at (1.5,6);
        %
        \filldraw[cw1] (A) -- (B) -- (C) -- (D) -- cycle;
        \draw (A) node[above right, xshift=0.1cm] {$F(-\infty, \infty)$} -- (B) -- (C) -- (D) -- cycle;
        \draw ($(C)!.91!(A)$) node[above right, xshift=0.1cm] {$C(-\infty, \infty)$} -- ($(D)!.91!(B)$) -- ($(A)!.91!(C)$) -- ($(B)!.91!(D)$) -- cycle;
        \draw ($(C)!.82!(A)$) node[above right, xshift=0.1cm] {$C^1(-\infty, \infty)$} -- ($(D)!.82!(B)$) -- ($(A)!.82!(C)$) -- ($(B)!.82!(D)$) -- cycle;
        \draw ($(C)!.73!(A)$) node[above right, xshift=0.1cm] {$C^m(-\infty, \infty)$} -- ($(D)!.73!(B)$) -- ($(A)!.73!(C)$) -- ($(B)!.73!(D)$) -- cycle;
        \draw ($(C)!.64!(A)$) node[above right, xshift=0.1cm] {$C^\infty(-\infty, \infty)$} -- ($(D)!.64!(B)$) -- ($(A)!.64!(C)$) -- ($(B)!.64!(D)$) -- cycle;
        \draw ($(C)!.55!(A)$) -- ($(D)!.55!(B)$) -- ($(A)!.55!(C)$) -- ($(B)!.55!(D)$) -- cycle;
        \node at (5.75,3) {$P_n$};
    \end{tikzpicture}
    \caption{La jerarquía de espacios funcionales}
    \label{JAJAIQPAPOAOSOOAKJS}
\end{figure}

El siguiente teorema proporciona una forma útil de crear un nuevo subespacio a partir de subespacios conocidos.

\begin{theorem}{}{}
    Si $W_1, W_2, \dots, W_r$ son subespacios de un espacio vectorial $V$, entonces la intersección de estos subespacios también es un subespacio de $V$.

    \tcblower
    \demostracion Sea $W$ la intersección de los subespacios $W_1, W_2, \dots, W_r$. Este conjunto no es vacío porque cada uno de estos subespacios contiene el vector cero de $V$, y por lo tanto, también lo hace su intersección. Así, queda por demostrar que $W$ está cerrado bajo la suma y la multiplicación por escalares. Para demostrar la cerradura bajo la suma, sean $\mathbf{u}$ y $\mathbf{v}$ vectores en $W$. Dado que $W$ es la intersección de $W_1, W_2, \dots, W_r$, se deduce que $\mathbf{u}$ y $\mathbf{v}$ también pertenecen a cada uno de estos subespacios. Además, como estos subespacios están cerrados bajo la suma y la multiplicación por escalares, también contienen los vectores $\mathbf{u} + \mathbf{v}$ y $\alpha \cdot \mathbf{u}$ para todo escalar $\alpha$, y por lo tanto, su intersección $W$ también los contiene. Esto prueba que $W$ está cerrado bajo la suma y la multiplicación por escalares.
\end{theorem}

\subsection{Conjuntos generadores}

\begin{definicion}{}{}
    Sea $V$ un espacio vectorial sobre $K$. Si $\mathbf{u}$ es un vector en un espacio vectorial $V$, entonces se dice que $\mathbf{u}$ es una \emph{combinación lineal} de los vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ en $V$ si $\mathbf{u}$ puede expresarse en la forma
    $$\mathbf{u} = k_1 \mathbf{v}_1 + k_2 \mathbf{v}_2 + \cdots + k_n \mathbf{v}_n,$$
    donde $k_1, k_2, \dots, k_n$ son escalares.
\end{definicion}

\begin{theorem}{}{CONMAPEQ}
    Sea $V$ un espacio vectorial sobre $K$. Si $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ es un conjunto no vacío de vectores en $V$, entonces:
    \begin{enumerate}[label=\alph*), topsep=6pt, itemsep=0pt]
        \item El conjunto $W$ de todas las combinaciones lineales posibles de los vectores en $S$ es un subespacio de $V$.
        \item El conjunto $W$ en el inciso anterior, es el subespacio más pequeño de $V$ que contiene a todos los vectores en $S$, en el sentido de que cualquier otro subespacio que contenga esos vectores también contiene a $W$.
    \end{enumerate}

    \newpage
    \demostracion
    \begin{enumerate}[label=\alph*), topsep=6pt, itemsep=0pt]
        \item Sea $W$ el conjunto de todas las combinaciones lineales posibles de los vectores en $S$. Debemos demostrar que $W$ está cerrado bajo la suma y la multiplicación por escalares. Para demostrar el cierre bajo la suma, sean
        $$\mathbf{x}_1 = a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \cdots + a_n\mathbf{v}_n \quad \text{ y } \quad \mathbf{x}_2 = b_1\mathbf{v}_1 + b_2\mathbf{v}_2 + \cdots + b_n\mathbf{v}_n$$
        dos vectores en $W$. Se deduce que su suma puede escribirse como
        $$\mathbf{x}_1 + \mathbf{x}_2 = (a_1 + b_1) \mathbf{v}_1 + (a_2 + b_2) \mathbf{v}_2 + \cdots + (a_n + b_n) \mathbf{v}_n,$$
        que es una combinación lineal de los vectores en $S$. Por lo tanto, $W$ está cerrado bajo la suma. Se deja como ejercicio al lector la demostración de que $W$ también está cerrado bajo la multiplicación por escalares y, por ende, es un subespacio de $V$.
        \item Sea $W'$ cualquier subespacio de $V$ que contiene a todos los vectores en $S$. Dado que $W'$ está cerrado bajo la suma y la multiplicación por escalares, contiene todas las combinaciones lineales de los vectores en $S$ y, por ende, contiene a $W$.
    \end{enumerate}
\end{theorem}

\begin{definicion}{}{PRIM}
    Se dice que los vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ de un espacio vectorial $V$ \emph{generan} a $V$ si todo vector en $V$ se puede escribir como una combinación lineal de los mismos. Es decir, para todo $\mathbf{v} \in V$ existen escalares $a_1, a_2, \dots, a_n$ tales que
    $$\mathbf{v} = a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \cdots + a_n\mathbf{v}_n.$$
\end{definicion}

\begin{examplebox}{}{}
    Recordemos que los vectores unitarios estándar en $\RR[n]$ son
    $$\mathbf{e}_1 = (1, 0, 0, \dots, 0), \quad \mathbf{e}_2 = (0, 1, 0, \dots, 0), \quad \dots, \quad \mathbf{e}_n = (0, 0, 0, \dots, 1).$$
    Estos vectores generan $\RR[n]$ ya que cualquier vector $\mathbf{v} = (v_1, v_2, \dots, v_n)$ en $\RR[n]$ se puede expresar como
    $$\mathbf{v} = v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + \dots + v_n \mathbf{e}_n$$
    lo cual es una combinación lineal de $\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n$. Así, por ejemplo, los vectores
    $$\mathbf{i} = (1, 0, 0), \quad \mathbf{j} = (0, 1, 0), \quad \mathbf{k} = (0, 0, 1)$$
    generan $\RR[3]$ ya que cualquier vector $\mathbf{v} = (a, b, c)$ se puede expresar como
    $$\mathbf{v} = (a, b, c) = a(1, 0, 0) + b(0, 1, 0) + c(0, 0, 1) = a\mathbf{i} + b\mathbf{j} + c\mathbf{k}.$$
\end{examplebox}

\begin{definicion}{}{SEGU}
    El \emph{espacio generado} por $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\}$ es el conjunto de combinaciones lineales $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$. Es decir,
    $$\Gen (\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\}) = \{\mathbf{v} \mid \mathbf{v} = a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \dots + a_k\mathbf{v}_k\}$$
    donde $a_1, a_2, \dots, a_k$ son escalares arbitrarios.
\end{definicion}

En las definiciones \ref{definicion:PRIM} y \ref{definicion:SEGU} se utilizaron dos términos diferentes: “genera” y “espacio generado”. Se hace hincapié en que un conjunto de vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ \emph{genera} a $V$ (que es un verbo) si todo vector en $V$ se puede escribir como una combinación lineal de $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$. Por otro lado, el \emph{espacio generado} (que es un sustantivo) por los $n$ vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$ es el conjunto de combinaciones lineales de estos vectores.

Estos dos conceptos son diferentes, aún cuando los términos se parezcan. Por ejemplo, si consideramos $V = \RR[3]$,
\begin{itemize}
    \item Definición \ref{definicion:PRIM}: $\{\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3\}$ es un conjunto generador de $V$, porque cualquier vector en $\RR[3]$ se puede escribir como combinación lineal de estos vectores.\newpage
    \item Definición \ref{definicion:SEGU}: Si tomamos $\{\mathbf{e}_1, \mathbf{e}_2\}$, el espacio generado es el subespacio $\Gen (\{\mathbf{e}_1, \mathbf{e}_2\})$, que es un plano en $\RR[3]$.
\end{itemize}
La diferencia está en que la definición \ref{definicion:PRIM} cubre el espacio completo, mientras que la \ref{definicion:SEGU} describe subespacios.

\begin{examplebox}{}{POLIGENERA}
    Los polinomios $1, x, x^2, \dots, x^n$ generan el espacio vectorial $P_n$ definido en el ejemplo \ref{examplebox:EJEMPLOFIN}, ya que cada polinomio $\mathbf{p}$ en $P_n$ puede escribirse como
    $$\mathbf{p} = a_0 + a_1x + \cdots + a_nx^n,$$
    que es una combinación lineal de $1, x, x^2, \dots, x^n$. Podemos denotar esto como
    $$P_n = \Gen\left(\left\{1, x, x^2, \dots, x^n\right\}\right).$$
\end{examplebox}

\begin{examplebox}{}{}
    Consideremos el espacio vectorial de todas las matrices $2 \times 2$. Cada matriz $A$ se puede expresar como
    $$A = a \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix} + b \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix} + c \begin{bmatrix}
        0 & 0 \\
        1 & 0
    \end{bmatrix} + d \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix}$$
    que es una combinación lineal de las siguientes matrices
    $$E_1 = \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix}, \quad E_2 = \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix}, \quad E_3 = \begin{bmatrix}
        0 & 0 \\
        1 & 0
    \end{bmatrix}, \quad E_4 = \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix}.$$
    Al igual que el ejemplo anterior, podemos denotar esto como
    $$\mathcalm{M}_{2 \times 2} = \Gen\left(\left\{ \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix}, \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix}, \begin{bmatrix}
        0 & 0 \\
        1 & 0
    \end{bmatrix}, \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix} \right\}\right).$$
\end{examplebox}

\begin{examplebox}{}{}
    Consideremos el espacio vectorial $V$ de todas las soluciones de la ecuación diferencial lineal homogénea de segundo orden
    $$y'' + y = 0.$$
    Este espacio está formado por todas las funciones $y: \RR \longrightarrow \RR$ que satisfacen dicha ecuación. Se sabe que la solución general de esta ecuación es
    $$y(x) = A\cos (x) + B\sin (x), \quad \text{ con } A, B \in \RR .$$
    Esto significa que cada solución de la ecuación diferencial se puede expresar como una combinación lineal de las funciones $\cos (x)$ y $\sin (x)$. Esto se denota como
    $$V = \Gen(\{\cos (x), \sin (x)\}).$$
\end{examplebox}

\subsection{Conjuntos linealmente independientes}

\begin{definicion}{}{}
    Sea $V$ un espacio vectorial sobre $K$. Un subconjunto $S$ del espacio vectorial $V$ se llama \emph{linealmente dependiente} si existen vectores distintos $\mathbf{v}_1$, $\mathbf{v}_2$, $\dots$, $\mathbf{v}_n$ en $S$ y $a_1$, $a_2$, $\dots$, $a_n$ en $K$ no todos $0$ tales que
    $$a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_n \mathbf{v}_n = \mathbf{0}.$$
    En este caso también decimos que los vectores de $S$ son linealmente dependientes. En caso contrario, decimos que los vectores son \emph{linealmente independientes} y por tanto, $S$ también es linealmente independiente.
\end{definicion}

Más concretamente, si $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ es un conjunto de dos o más vectores en $V$, entonces $S$ se dice que es un \emph{conjunto linealmente independiente} si ningún vector en $S$ puede expresarse como una combinación lineal de los demás. En caso contrario, $S$ se considera un \emph{conjunto linealmente dependiente}.

\newpage

\begin{theorem}{}{}
    Dos vectores en un espacio vectorial son linealmente dependientes si y solo si uno de ellos es un múltiplo escalar del otro.

    \tcblower
    \demostracion Primero supongamos que $\mathbf{v}_2 = c \mathbf{v}_1$ para algún escalar $c \neq 0$. Entonces $c \mathbf{v}_1 - \mathbf{v}_2 = \mathbf{0}$ y $\mathbf{v}_1$ y $\mathbf{v}_2$ son linealmente dependientes. Por otro lado, supongamos que $\mathbf{v}_1$ y $\mathbf{v}_2$ son linealmente dependientes. Entonces existen escalares $c_1$ y $c_2$, al menos uno distinto de cero, tales que
    $$c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 = \mathbf{0}.$$
    Si $c_1 \neq 0$, entonces dividiendo entre $c_1$ se obtiene
    $$\mathbf{v}_1 + \left(\frac{c_2}{c_1}\right) \mathbf{v}_2 = \mathbf{0},$$
    o sea,
    $$\mathbf{v}_1 = \left(-\frac{c_2}{c_1}\right) \mathbf{v}_2.$$
    Es decir, $\mathbf{v}_1$ es un múltiplo escalar de $\mathbf{v}_2$. Si $c_1 = 0$, entonces $c_2 \neq 0$ y, por lo tanto,
    $$\mathbf{v}_2 = \mathbf{0} = 0 \mathbf{v}_1.$$
\end{theorem}

\begin{examplebox}{}{}
    Los vectores
    $$\mathbf{v}_1 = \begin{pmatrix*}[r] 2 \\ -1 \\ 0 \end{pmatrix*} \quad \text{ y } \quad \mathbf{v}_2 = \begin{pmatrix*}[r] -6 \\ 3 \\ 0 \end{pmatrix*}$$
    son linealmente dependientes ya que $\mathbf{v}_2 = -3\mathbf{v}_1$.
\end{examplebox}

\begin{theorem}{}{}
    Un conjunto de $n$ vectores en $\RR[m]$ es siempre linealmente dependiente si $n > m$.

    \tcblower
    \demostracion Se deja como ejercicio al lector.
\end{theorem}

\begin{examplebox}{}{POLILINEAL}
    Demostrar que los polinomios
    $$1, x, x^2, \dots, x^n$$
    forman un conjunto linealmente independiente en $P_n$.

    \tcblower
    \solucion Por conveniencia, denotemos los polinomios como
    $$\mathbf{p}_0 = 1, \mathbf{p}_1 = x, \mathbf{p}_2 = x^2, \dots, \mathbf{p}_n = x^n.$$
    Debemos demostrar que los únicos coeficientes que satisfacen la ecuación vectorial
    \begin{equation}
        a_0\mathbf{p}_0 + a_1\mathbf{p}_1 + a_2\mathbf{p}_2 + \cdots + a_n\mathbf{p}_n = \mathbf{0} \label{eq:vectorial}
    \end{equation}
    son
    $$a_0 = a_1 = a_2 = \cdots = a_n = 0.$$
    La ecuación \eqref{eq:vectorial} es equivalente a la afirmación de que
    \begin{equation}
        a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = 0 \label{eq:polinomial}
    \end{equation}
    para todo $x \in (-\infty, \infty)$. Por lo tanto, debemos demostrar que esto es cierto si y solo si cada coeficiente en \eqref{eq:polinomial} es igual a cero. Para ver que esto es cierto, recordemos del álgebra que un polinomio no nulo de grado $n$ tiene como máximo $n$ raíces distintas. Siendo este el caso, cada coeficiente en \eqref{eq:polinomial} debe ser igual a cero; de lo contrario, el lado izquierdo de la ecuación sería un polinomio no nulo con un número infinito de raíces, lo cual es una contradicción. Por lo tanto, la ecuación \eqref{eq:vectorial} solo tiene la solución trivial.
\end{examplebox}

\newpage

\begin{theorem}{}{}
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Un conjunto finito que contiene el vector $\mathbf{0}$ es linealmente dependiente.
        \item Un conjunto con exactamente un vector, es linealmente independiente si y solo si ese vector es distinto de $\mathbf{0}$.
    \end{enumerate}

    \tcblower
    \demostracion
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Para cualquier conjunto de vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ en un espacio vectorial, el conjunto $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n, \mathbf{0}\}$ es linealmente dependiente, ya que la ecuación
        $$0\mathbf{v}_1 + 0\mathbf{v}_2 + \cdots + 0\mathbf{v}_n + 1(\mathbf{0}) = \mathbf{0}$$
        expresa a $\mathbf{0}$ como una combinación lineal de los vectores en $S$ con coeficientes que no son todos iguales a $0$.
        \item Se deja como ejercicio al lector.
    \end{enumerate}
\end{theorem}

% Cositas

\newpage

\subsection{Base y dimensión}

\begin{definicion}{}{}
    Decimos que un conjunto finito de vectores $\{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \}$ es una \emph{base} para un espacio vectorial $V$ si:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item $\{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \}$ es linealmente independiente,
        \item $\{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \}$ genera a $V$.
    \end{enumerate}
\end{definicion}

Si consideras una base como la descripción de un sistema de coordenadas para un espacio vectorial $V$ de dimensión finita, entonces la parte (i) de esta definición garantiza que no existe ninguna interrelación entre los vectores base, la parte (ii) garantiza que hay suficientes vectores base para proporcionar coordenadas a todos los vectores en $V$. A continuación, se presentan algunos ejemplos.

\begin{examplebox}{}{}
    Demostrar que $S = \left\{1, x, x^2, \dots, x^n\right\}$ es una base para el espacio vectorial $P_n$ de polinomios de grado $n$ o menor.
    
    \tcblower
    \solucion Debemos demostrar que los polinomios en $S$ son linealmente independientes y generan $P_n$. Denotemos estos polinomios como
    $$\mathbf{p}_0 = 1, \mathbf{p}_1 = x, \mathbf{p}_2 = x^2, \dots, \mathbf{p}_n = x^n.$$
    Mostramos en el ejemplo \ref{examplebox:POLIGENERA} que estos vectores generan $P_n$ y en el ejemplo \ref{examplebox:POLILINEAL} que son linealmente independientes. Por lo tanto, forman una base para $P_n$ que llamamos la \emph{base estándar} de $P_n$.
\end{examplebox}

\begin{definicion}{}{}
    Sea $V$ un espacio vectorial sobre $K$. Se llama dimensión del espacio vectorial $V$, a la cardinalidad de la base de $V$, y se denotará por $\Dim V$.
\end{definicion}
\infoBulle{Recordemos que si $A$ es un conjunto no vacío, se define la \emph{cardinalidad} del conjunto $A$, denotado por $|A|$, como el número de elementos de $A$. Además, se dice que el conjunto $A$ es finito si $|A| < \infty$. En caso contrario, se dice que el conjunto es infinito. Por ejemplo, el conjunto $\NN$.}

Recordemos que la dimensión de un espacio vectorial se define como la cantidad de vectores linealmente independientes necesarios para generar todo el espacio. En el caso del vector cero, su dimensión es $0$ porque no puede generar ningún otro vector aparte de sí mismo mediante combinaciones lineales. Para entenderlo mejor, considera que cualquier vector $\mathbf{v}$ en un espacio vectorial $V$ puede ser expresado como:
$$\mathbf{v} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n$$
donde $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ son vectores linealmente independientes en $V$, y $c_1, c_2, \dots, c_n$ son escalares. Ahora, si consideramos el vector cero, no importa cuánto intentemos expresarlo como una combinación lineal de otros vectores $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$, siempre obtendremos $\mathbf{0}$ como resultado, ya que cualquier escalar multiplicado por $\mathbf{0}$ sigue siendo $\mathbf{0}$.

Por lo tanto, la dimensión del espacio generado por el vector cero es igual a $0$, ya que no requiere ningún otro vector para ser generado, y no puede generar ningún otro vector aparte de sí mismo.

\newpage

~
% Cositas

\newpage

\section{Espacios vectoriales normados}

Los espacios vectoriales de interés en el análisis abstracto y en aplicaciones tienen una estructura mucho más rica que la descrita únicamente por los ocho axiomas principales. Los axiomas de espacio vectorial solo describen las propiedades algebraicas de los elementos del espacio: suma, multiplicación por escalares y combinaciones de estas. Sin embargo, faltan los conceptos topológicos como apertura, clausura, convergencia y completitud. Estos conceptos pueden introducirse mediante una medida de distancia en el espacio.

\begin{definicion}{}{normaV}
    Sea $V$ un espacio vectorial sobre $K$. Se define la \emph{norma} en $V$ como una función $\| \phantom{x} \| : V \longrightarrow \RR$ que satisface las siguientes propiedades:
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item $\| \mathbf{x} \| \geq 0$, para todo $\mathbf{x} \in V$.
        \item $\| \mathbf{x} \| = 0$ si y solo si $\mathbf{x} = \mathbf{0}$.
        \item $\| \alpha \mathbf{x} \| = |\alpha| \| \mathbf{x} \|$, para todo escalar $\alpha$ y $\mathbf{x} \in V$.
        \item $\| \mathbf{x} + \mathbf{y} \| \leq \| \mathbf{x} \| + \| \mathbf{y} \|$, para todo $\mathbf{x}$, $\mathbf{y} \in V$. \hfill (desigualdad del triángulo)
    \end{enumerate}
\end{definicion}

\begin{examplebox}{}{}
    En $\RR$, sea $\| \mathbf{x} \| = |x|$, probemos entonces que $\| \phantom{x} \|$ cumple las cuatro propiedades establecidas en la definición \ref{definicion:normaV}.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Procedamos por casos.
        \begin{enumerate}
            \item Si $x \geq 0$, se define $|x| = x$. Como $x \geq 0$, se tiene $|x| \geq 0$.
            \item Si $x < 0$, se define $|x| = -x$. Como $x$ es negativo, $-x$ es positivo, es decir, $|x| \geq 0$.
        \end{enumerate}
        En ambos casos se concluye que $|x| \geq 0$ para todo $x \in \RR$.
        \item Del inciso anterior, $|x| = 0$ no puede ocurrir a menos que $x = 0$.
        \item Consideremos dos casos respecto a $\alpha$.
        \begin{enumerate}
            \item Sea $\alpha \geq 0$, entonces $|\alpha| = \alpha$. Si $x \geq 0$, entonces $\alpha x \geq 0$ y
            $$|\alpha x| = \alpha x = \alpha |x|.$$
            Si $x < 0$, entonces $\alpha x \leq 0$ y
            $$|\alpha x| = -(\alpha x) = \alpha (-x) = \alpha |x|.$$
            \item Sea $\alpha < 0$, entonces $|\alpha| = -\alpha$. Si $x \geq 0$, entonces $\alpha x \leq 0$ y
            $$|\alpha x| = -(\alpha x) = (-\alpha) x = |\alpha| |x|.$$
            Si $x < 0$, entonces $\alpha x \geq 0$ y
            $$|\alpha x| = \alpha x = (-\alpha)(-x) = |\alpha| |x|.$$
        \end{enumerate}
        En todos los casos se obtiene que $|\alpha x| = |\alpha| |x|$.
        \item Consideremos que
        $$(|x| + |y|)^2 = x^2 + 2|x||y| + y^2,$$
        y
        $$|x + y|^2 = (x + y)^2 = x^2 + 2xy + y^2.$$
        Dado que $2xy \leq 2|x||y|$, se tiene $|x + y|^2 \leq (|x| + |y|)^2$. Al tomar la raíz cuadrada de ambos lados se concluye que
        $$|x + y| \leq |x| + |y|.$$
    \end{enumerate}
    Dado que el valor absoluto en $\RR$ cumple las cuatro propiedades anteriores, concluimos que $|\phantom{x}|$ es una norma en $\RR$.
\end{examplebox}

\newpage

Hemos demostrado que el valor absoluto en $\RR$ cumple con las propiedades fundamentales de una norma. A partir de esta propiedad, se puede extender la idea de norma a espacios de mayor dimensión, como $\RR[n]$, y demostrar que ciertas definiciones, como la norma $\| \phantom{x} \|_1$, también cumplen con los requisitos necesarios para ser consideradas normas en este contexto.

\begin{examplebox}{}{}
    Sea $\| \phantom{x} \|_1$ la función definida en $\RR[n]$ por
    $$\| \mathbf{x} \|_1 = |x_1| + |x_2| + \cdots + |x_n|,$$
    para cada $\mathbf{x} = (x_1, x_2, \dots, x_n)$. Utilizando las propiedades del valor absoluto como base, demostremos que $\| \phantom{x} \|_1$ cumple las cuatro propiedades establecidas en la definición \ref{definicion:normaV}.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Como el valor absoluto es siempre no negativo, se tiene que $|x_i| \geq 0$ para cada $i \in \{ 1, 2, \dots, n \}$. Al sumar estos valores, se obtiene
        $$\| \mathbf{x} \|_1 = |x_1| + |x_2| + \cdots + |x_n| \geq 0.$$
        \item Si $\| \mathbf{x} \|_1 = 0$, entonces
        $$|x_1| + |x_2| + \cdots + |x_n| = 0.$$
        Como cada término $|x_i| \geq 0$, esto solo es posible si $|x_i| = 0$, es decir, $x_i = 0$ para cada $i \in \{ 1, 2, \dots, n \}$. Por lo tanto, $\mathbf{x} = \mathbf{0}$. Recíprocamente, si $\mathbf{x} = \mathbf{0}$, entonces $x_i = 0$ para cada $i \in \{ 1, 2, \dots, n \}$, lo que implica que $\| \mathbf{x} \|_1 = 0$.
        \item Sea $\mathbf{x} = (x_1, x_2, \dots, x_n)$, entonces $\alpha \mathbf{x} = (\alpha x_1, \alpha x_2, \dots, \alpha x_n)$. Por la propiedad del valor absoluto, se tiene que $|\alpha x_i| = |\alpha| |x_i|$, por lo que
        \begin{align*}
            \| \alpha \mathbf{x} \|_1 & = |\alpha x_1| + |\alpha x_2| + \cdots + |\alpha x_n| \\
            & = |\alpha| |x_1| + |\alpha| |x_2| + \cdots + |\alpha| |x_n| \\
            & = |\alpha| (|x_1| + |x_2| + \cdots + |x_n|) \\
            & = |\alpha| \| \mathbf{x} \|_1
        \end{align*}
        \item Tenemos que $\mathbf{x} + \mathbf{y} = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)$. Al usar la desigualdad triangular para cada $i \in \{ 1, 2, \dots, n \}$ en $\RR$,
        $$|x_i + y_i| \leq |x_i| + |y_i|$$
        obtenemos
        \begin{align*}
            \| \mathbf{x} + \mathbf{y} \|_1 & = |x_1 + y_1| + |x_2 + y_2| + \cdots + |x_n + y_n| \\
            & \leq |x_1| + |y_1| + |x_2| + |y_2| + \cdots + |x_n| + |y_n| \\
            & = |x_1| + |x_2| + \cdots + |x_n| + |y_1| + |y_2| + \cdots + |y_n| \\
            & = \| \mathbf{x} \|_1 + \| \mathbf{y} \|_1
        \end{align*}
    \end{enumerate}
    Dado que $\| \phantom{x} \|_1$ en $\RR[n]$ cumple las cuatro propiedades anteriores, concluimos que $\| \phantom{x} \|_1$ es una norma en $\RR[n]$.
\end{examplebox}

Utilizando las propiedades de los números reales como base, procederemos a mostrar que la norma $\| \phantom{x} \|_2$ en $\RR[n]$, definida como la raíz cuadrada de la suma de los cuadrados de las componentes de un vector, también satisface las cuatro condiciones esenciales que definen a una norma en un espacio vectorial.

\begin{examplebox}{}{}
    Sea $\| \phantom{x} \|_2$ la función definida en $\RR[n]$ por
    $$\| \mathbf{x} \|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2},$$
    para cada $\mathbf{x} = (x_1, x_2, \dots, x_n)$. Demostremos que $\| \phantom{x} \|_2$ cumple las cuatro propiedades establecidas en la definición \ref{definicion:normaV}.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Dado que para cada $i \in \{ 1, 2, \dots, n \}$, $x_i^2 \geq 0$, se sigue que
        $$x_1^2 + x_2^2 + \dots + x_n^2 \geq 0.$$
        Esto nos lleva a que
        $$\| \mathbf{x} \|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2} \geq 0.$$
        \item Del inciso anterior, si $\| \mathbf{x} \|_2 = 0$, entonces
        $$x_1^2 + x_2^2 + \dots + x_n^2 = 0.$$
        Esto ocurre si y solo si $x_i = 0$ para cada $i \in \{ 1, 2, \dots, n \}$, es decir, $\mathbf{x} = \mathbf{0}$.
        \item Sabiendo que para cualquier escalar, se cumple $\sqrt{\alpha^2} = |\alpha|$, tenemos que
        \begin{align*}
            \|\alpha \mathbf{x}\|_2 & = \sqrt{(\alpha x_1)^2 + (\alpha x_2)^2 + \dots + (\alpha x_n)^2} \\
            & = \sqrt{\alpha^2 x_1^2 + \alpha^2 x_2^2 + \dots + \alpha^2 x_n^2} \\
            & = \sqrt{\alpha^2 \left(x_1^2 + x_2^2 + \dots + x_n^2\right)} \\
            & = |\alpha| \sqrt{x_1^2 + x_2^2 + \dots + x_n^2} \\
            & = |\alpha| \| \mathbf{x} \|_2
        \end{align*}
        \item Tenemos que $\mathbf{x} + \mathbf{y} = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)$ y por definición,
        $$\| \mathbf{x} + \mathbf{y} \|_2 = \sqrt{(x_1 + y_1)^2 + (x_2 + y_2)^2 + \cdots + (x_n + y_n)^2}.$$
        Usamos la desigualdad básica
        $$(x_i + y_i)^2 \leq 2\left(x_i^2 + y_i^2\right),$$
        la cual se obtiene de
        $$0 \leq (x_i - y_i)^2 = x_i^2 - 2x_iy_i + y_i^2.$$
        Reordenando y sumando para todas las componentes,
        $$2 \sum_{i=1}^{n} x_i y_i \leq \sum_{i=1}^{n} x_i^2 + \sum_{i=1}^{n} y_i^2.$$
        Lo que conlleva a que
        \begin{align*}
            \| \mathbf{x} + \mathbf{y} \|_2 & = \sqrt{\sum_{i=1}^{n} x_i^2 + 2 \sum_{i=1}^{n} x_i y_i + \sum_{i=1}^{n} y_i^2} \\
            & \leq \sqrt{2\sum_{i=1}^{n} x_i^2 + 2\sum_{i=1}^{n} y_i^2} \\
            & = \sqrt{2 \left( \sum_{i=1}^{n} x_i^2 + \sum_{i=1}^{n} y_i^2 \right)} \\
            & \leq \sqrt{\sum_{i=1}^{n} x_i^2 + \sum_{i=1}^{n} y_i^2} \\
            & \leq \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} + \sqrt{y_1^2 + y_2^2 + \cdots + y_n^2} \\
            & = \| \mathbf{x} \|_1 + \| \mathbf{y} \|_1
        \end{align*}
    \end{enumerate}
    Dado que $\| \phantom{x} \|_2$ en $\RR[n]$ cumple las cuatro propiedades anteriores, concluimos que $\| \phantom{x} \|_2$ es una norma en $\RR[n]$.
\end{examplebox}

\newpage

A continuación, exploraremos la norma infinita, que se define como el valor máximo de los valores absolutos de las componentes del vector. Veremos cómo esta norma, a pesar de su distinta formulación, también satisface las cuatro propiedades fundamentales que caracterizan a una norma en un espacio vectorial.

\begin{examplebox}{}{}
    Sea $\| \phantom{x} \|_{\infty}$ la función definida en $\RR[n]$ por
    $$\| \mathbf{x} \|_{\infty} = \max \{ |x_1|, |x_2|, \dots, |x_n| \},$$
    para cada $\mathbf{x} = (x_1, x_2, \dots, x_n)$. Demostremos que $\| \phantom{x} \|_{\infty}$ cumple las cuatro propiedades establecidas en la definición \ref{definicion:normaV}.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Como el valor absoluto es siempre no negativo, se tiene que $|x_i| \geq 0$ para cada $i \in \{ 1, 2, \dots, n \}$. De lo anterior dicho, se concluye que
        $$\| \mathbf{x} \|_{\infty} = \max \{ |x_1|, |x_2|, \dots, |x_n| \} \geq 0.$$
        \item Si $\| \mathbf{x} \|_{\infty} = 0$, entonces
        $$\max \{ |x_1|, |x_2|, \dots, |x_n| \} = 0.$$
        Como el máximo de un conjunto de números no negativos es cero solo si todos los elementos son cero, se deduce que para cada $i \in \{ 1, 2, \dots, n \}$, $|x_i| = 0$, es decir, $x_i = 0$. Por lo tanto, $\mathbf{x} = \mathbf{0}$. Recíprocamente, si $\mathbf{x} = \mathbf{0}$, entonces $x_i = 0$ para cada $i \in \{ 1, 2, \dots, n \}$, lo que implica que $\| \mathbf{x} \|_\infty = 0$.
        \item Sea $\mathbf{x} = (x_1, x_2, \dots, x_n)$, entonces $\alpha \mathbf{x} = (\alpha x_1, \alpha x_2, \dots, \alpha x_n)$. Por la propiedad del valor absoluto, se tiene que $|\alpha x_i| = |\alpha| |x_i|$, por lo que
        \begin{align*}
            \| \alpha \mathbf{x} \|_{\infty} & = \max \{ |\alpha x_1|, |\alpha x_2|, \dots, |\alpha x_n| \} \\
            & = \max \{ |\alpha| |x_1|, |\alpha| |x_2|, \dots, |\alpha| |x_n| \} \\
            & = |\alpha| \max \{ |x_1|, |x_2|, \dots, |x_n| \} \\
            & = |\alpha| \| \mathbf{x} \|
        \end{align*}
        \item Se deja como ejercicio al lector.
    \end{enumerate}
    Dado que $\| \phantom{x} \|_{\infty}$ en $\RR[n]$ cumple las cuatro propiedades anteriores, concluimos que $\| \phantom{x} \|_{\infty}$ es una norma en $\RR[n]$.
\end{examplebox}

Ahora veremos una norma en un espacio vectorial diferente.

\begin{examplebox}{}{}
    Sea $\| \phantom{x} \|$ la función definida en $C[a, b]$ por
    $$\| \mathbf{f} \| = \max_{t \in [a, b]} |f(t)|,$$
    para cada $\mathbf{f} = f(t)$. Utilizando las propiedades del valor absoluto, demostremos que $\| \phantom{x} \|$ cumple las cuatro propiedades establecidas en la definición \ref{definicion:normaV}.
    \begin{enumerate}[label=\roman*), topsep=6pt, itemsep=0pt]
        \item Dado que el valor absoluto es siempre no negativo, $\| \mathbf{f} \| \geq 0$ para todo $\mathbf{f}$.
        \item Del inciso anterior, $\| \mathbf{f} \| = 0$ si y solo si $|f(t)| = 0$ para todo $t \in [a, b]$, lo cual ocurre solo cuando $f(t) = 0$ para todos los $t \in [a, b]$, es decir, $\mathbf{f} = \mathbf{0}$.
        \item Si $f \in C[a, b]$ y $\alpha \in \RR$, entonces
        \begin{align*}
            \| \alpha \mathbf{f} \| & = \max_{t \in [a, b]} |\alpha f(t)| \\
            & = |\alpha| \max_{t \in [a, b]} |f(t)| \\
            & = |\alpha| \| \mathbf{f} \|.
        \end{align*}
        \item Se deja como ejercicio al lector.
    \end{enumerate}
    Dado que $\| \phantom{x} \|$ en $C[a, b]$ cumple las cuatro propiedades anteriores, concluimos que $\| \phantom{x} \|$ es una norma en $C[a, b]$.
\end{examplebox}

\newpage

\begin{definicion}{}{}
    Un \emph{espacio normado} es un espacio vectorial $V$ con una norma definida en $V$.
\end{definicion}

\newpage

\begin{theorem}{}{}
    \TituloBox{Desigualdad de Cauchy-Schwarz:} Sea $V$ un espacio vectorial sobre $K$ con producto interno $\langle \, , \rangle$. Sean $\mathbf{x}$, $\mathbf{y} \in V$, entonces se cumple
    $$| \langle \mathbf{x}, \mathbf{y} \rangle | \leq \| \mathbf{x} \| \| \mathbf{y} \|.$$

    \tcblower
    \demostracion Sean $\mathbf{x}$, $\mathbf{y} \in V$, con $\mathbf{y} \neq \mathbf{0}$ y sea $\lambda \in K$. Consideremos al vector $\mathbf{x} + \lambda \mathbf{y}$, entonces se tiene
    \begin{align*}
        0 \leq \| \mathbf{x} + \lambda \mathbf{y} \|^2 & = \langle \mathbf{x} + \lambda\mathbf{y}, \mathbf{x} + \lambda\mathbf{y} \rangle \\
        & = \langle \mathbf{x}, \mathbf{x} + \lambda \mathbf{y} \rangle + \langle \lambda \mathbf{y}, \mathbf{x} + \lambda\mathbf{y} \rangle \\
        & = \langle \mathbf{x}, \mathbf{x} \rangle + \langle \mathbf{x}, \lambda \mathbf{y} \rangle + \langle \lambda \mathbf{y}, \mathbf{x} \rangle + \langle \lambda \mathbf{y}, \lambda \mathbf{y} \rangle \\
        & = \langle \mathbf{x}, \mathbf{x} \rangle + \overline{\lambda} \langle \mathbf{x}, \mathbf{y} \rangle + \lambda \langle \mathbf{y}, \mathbf{x} \rangle + \lambda \langle \mathbf{y}, \lambda \mathbf{y} \rangle \\
        & = \langle \mathbf{x}, \mathbf{x} \rangle + \overline{\lambda} \langle \mathbf{x}, \mathbf{y} \rangle + \lambda \overline{\langle \mathbf{x}, \mathbf{y} \rangle} + \lambda\overline{\lambda} \langle \mathbf{y}, \mathbf{y} \rangle \\
        & = \| \mathbf{x} \|^2 + \overline{\lambda} \langle \mathbf{x}, \mathbf{y} \rangle + \lambda \overline{\langle \mathbf{x}, \mathbf{y} \rangle} + |\lambda|^2 \| \mathbf{y} \|^2
    \end{align*}
    Así,
    $$0 \leq \| \mathbf{x} + \lambda \mathbf{y} \|^2 = \| \mathbf{x} \|^2 + \overline{\lambda} \langle \mathbf{x}, \mathbf{y} \rangle + \lambda \overline{\langle \mathbf{x}, \mathbf{y} \rangle} + |\lambda|^2 \| \mathbf{y} \|^2.$$
    Entonces
    \begin{equation}
        0 \leq \| \mathbf{x} \|^2 + \overline{\lambda} \langle \mathbf{x}, \mathbf{y} \rangle + \lambda \overline{\langle \mathbf{x}, \mathbf{y} \rangle} + |\lambda|^2 \| \mathbf{y} \|^2, \label{desigualdad-cauchy}
    \end{equation}
    lo cual se cumple para toda $\lambda \in K$, en particular para $\displaystyle \lambda = \frac{(-1)\langle \mathbf{x}, \mathbf{y} \rangle}{\| \mathbf{y} \|^2}$. Además, es evidente que $\displaystyle \overline{\lambda} = \frac{(-1)\overline{\langle \mathbf{x}, \mathbf{y} \rangle}}{\| \mathbf{y} \|^2}$. Así que,
    $$|\lambda|^2 = \frac{(-1)\langle \mathbf{x}, \mathbf{y} \rangle}{\| \mathbf{y} \|^2} \cdot \frac{(-1)\overline{\langle \mathbf{x}, \mathbf{y} \rangle}}{\| \mathbf{y} \|^2} = \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^4}.$$
    Sustituyendo $\lambda$, $\overline{\lambda}$ y $|\lambda|^2$ en la expresión \eqref{desigualdad-cauchy}, se sigue que
    \begin{align*}
        0 & \leq \| \mathbf{x} \|^2 - \frac{\overline{\langle \mathbf{x}, \mathbf{y} \rangle}}{\| \mathbf{y} \|^2} \langle \mathbf{x}, \mathbf{y} \rangle - \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\| \mathbf{y} \|^2} \overline{\langle \mathbf{x}, \mathbf{y} \rangle} + \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^4} \| \mathbf{y} \|^2 \\
        & = \| \mathbf{x} \|^2 - \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^2} - \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^2} + \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^2} \\
        & = \| \mathbf{x} \|^2 - \frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^2}
    \end{align*}
    Entonces
    $$\frac{|\langle \mathbf{x}, \mathbf{y} \rangle|^2}{\| \mathbf{y} \|^2} \leq \| \mathbf{x} \|^2,$$
    de donde se sigue que
    $$|\langle \mathbf{x}, \mathbf{y} \rangle|^2 \leq \| \mathbf{x} \|^2 \| \mathbf{y} \|^2.$$
    Así, finalmente obtenemos que
    $$|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \| \mathbf{x} \| \| \mathbf{y} \|.$$
\end{theorem}

\begin{corollary}{}{}
    Si $\mathbf{x}$ e $\mathbf{y}$ son linealmente dependientes, entonces
    $$|\langle \mathbf{x}, \mathbf{y} \rangle| = \| \mathbf{x} \| \| \mathbf{y} \|.$$
    Si $\mathbf{x}$ e $\mathbf{y}$ son linealmente independientes, entonces
    $$|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \| \mathbf{x} \| \| \mathbf{y} \|.$$
\end{corollary}